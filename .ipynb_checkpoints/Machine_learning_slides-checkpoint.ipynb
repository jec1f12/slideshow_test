{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:d4234e617b3e4f01afcdc7fb35af227438046de519e2d05e474a12e095c72f64"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Work update#   \n",
      "##Josh Campbell## \n",
      "###27/05/14###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Outline##\n",
      "\n",
      "###Quinacridone CSP results###\n",
      "\n",
      "###Structure generator vs Polymorph Predictor round 2###\n",
      "\n",
      "###Machine learning introduction###\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Quinacridone"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Previous results presented showed that we had located all four experimental polymorphs ($\\alpha,\\alpha',\\beta,\\gamma$), and while we had them in the correct relative order their location within the search differed to the original CSP study. Our CSP conincided with the addition of some convergence tools to CSPy. Using them revealed the search was poorly converged so I've been bumping structures up slowly and checking convergence. \n",
      "\n",
      "\n",
      "<img src=\"quinacridone.png\" style=\"width: 300px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Convergence tools ##\n",
      "\n",
      "Added by Dave to process_data.py. To get the correct data for plotting you must add the option `--record_cluster_matches` to your clustering command. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "`python ~/CSPy/cspy/process_data.py -rp your_clustered.p --read_clustered_instances --convergence_graph1` **or** `--convergence_graph2`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Both these options produce a text file with the correct data, which will get integrated into any wider plotting module. Once plotted they look like this..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "###Convergence graph 1###\n",
      "\n",
      "Showing the number of unique structures versus successful minimisations, this allows you to see when structure generation is struggling to generate new structures. Text file comes with energy cutoff data as well. Graph below is for quinacridone $P2_1/c$ after 6000 structures.\n",
      "\n",
      "<img src=\"quin_test_14_cg1.png\"  style=\"width: 600px;\"/>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Convergence graph 2 ##\n",
      "\n",
      "Shows the rank of structures against their sobol seed, structures that are clustered together go with their lowest rank. Allows you to easily see where there are gaps in your low energy structures. This is also from quinacridone $P2_1/c$ after 6000 structures.\n",
      "\n",
      "<img src=\"quin_test_14_cg2.png\"  style=\"width: 600px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Back to the show ##\n",
      "\n",
      "The CSP methodology was the exact same as previously except these results are from 6000 structures per spacegroup. Below is the plot of lattice energy vs density for structures upto 20 kj/mol above the minimum. Of the top 20 structures 12 were generated after the initial 2000 (and 19 from structures that intially failed).\n",
      "\n",
      "<img src=\"quin_combined.png\" style=\"width: 600px;\"/>\n",
      "\n",
      "Our intial ranking after 2000 structures: $\\gamma$ = 3rd, $\\beta$ = 16th, $\\alpha$ = 27th  \n",
      "6000 structures: $\\gamma$ = 29th, $\\beta$ = 123rd, $\\alpha$ = 160th  \n",
      "PP study = $\\gamma$ = 1st, $\\beta$ = 16th, $\\alpha$ = 12th\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Packing analysis ##\n",
      "\n",
      "Most low energy structures of quinacridone show slight twists on the $\\gamma$ or $\\beta$ polymorphs (the lowest typically show gamma-like packing) and in fact the 2nd lowest energy structure is a match for $\\gamma$ just not the best one. $\\gamma$ like structures are characterised by their hydrogen bonding motif, 4 hydrogen bonds to 4 other molecules, giving the criss-cross motif.\n",
      "\n",
      "<img src=\"gamma_poly.png\" style=\"width: 600px;\"/>\n",
      "<img src=\"quin_gm.png\" style=\"width: 600px:\"/>\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## What next? ##\n",
      "\n",
      "Search is still not fully converged but first will be attempting a reranking of the experimental matches using a polarisable continuum model. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "SG vs PP round 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Previously..##\n",
      "\n",
      "In my last talk I presented a comparison of the sampling between SG and PP. Structure generator performed very well for some molecules (5A and TT in particular), missed important structures (TP, P, 5D, 7A) or went down pretty badly (7A,7D). Armed with our new convergence tools I've been steadily increasing the number of structures and checking matches. For singular misses I first increased the number of structures in that spacegroup, then increased it for all of them.\n",
      "\n",
      "| Molecule      | Matches          | Notable Misses  |\n",
      "| ------------- |:-------------:| -----:|\n",
      "| 5A    | 199/237 | 15 |\n",
      "| 5D     | 196/297|  1,2 |\n",
      "| 7A | 22/35      | 2,3,5,10 |\n",
      "|7D | 6/56 | Many! |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The new table...\n",
      "\n",
      "| Molecule      | Matches          | Notes  | No of structs |\n",
      "| ------------- |:-------------:| -----:|\n",
      "| 5A    | 211/237 | Looking good! (15 was actually a seg fault) | 4000\n",
      "| 5D     | 196/297|  1 and 2 actually clustered together in mercury, new global minimum is a match for it rms 0.299  | 5000\n",
      "| 7A | 27/35      | 20/20! 2nd and 3rd matches found 3-4000 ss, 5, 10 seg faulted| 10000\n",
      "|7D | 9/56 | Many! A lot of seg faulting structures, really need to go through them by hand | 10000"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Machine learning "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Definition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Machine learning is concerned with the design of programs/algorithims that automatically improve with experience. Tom Mitchell in his 1997 book Machine Learning gives a slightly more formal definition:\n",
      "\n",
      "<img src=\"machine_learning.jpg\" style=\"width: 300px;\" />\n",
      "\n",
      "> \"A computer program is said to **learn** from experience *E* with respect\n",
      "> to some class of tasks *T* and performance measure *P*, if its performance\n",
      "> at tasks in *T*, as measured by *P*, improves with experience *E*.\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Forming your problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      " \n",
      "The first step in forming your learning program is identifying your *T*, *P* and *E*:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "**A checkers game**:  \n",
      "   Task *T*: playing checkers  \n",
      "   Performance measure *P*: percent of games won against opponents  \n",
      "   Training experience *E*: playing practice games against itself  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "**A handwriting recognition problem**:  \n",
      "*T*: recognizing and classifying handwritten words with images  \n",
      "*P*: percentage of words correctly classified  \n",
      "*E*: a database of words already classified  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "**A robot driving problem**:   \n",
      "*T*: driving on a motorway using vision sensors  \n",
      "*P*: average distance travelled before error (as judged by a human observer)  \n",
      "*E*: a sequence of images and steering commands recorded while observing a human driver  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Or..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "**A molecular design and property optimisation...problem**:  \n",
      "*T*: design and CSP of a molecule  \n",
      "*P*: charge mobility of proposed molecule  \n",
      "*E*: molecules with known charge mobilities   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Training experiences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The first design choice is the type of training experience available to the learning program **Direct** or **Indirect**.\n",
      "\n",
      "For a checkers program direct experience could be learning from individual board states with the correct move for each.\n",
      "\n",
      "An indirect experience could be move sequences and final outcomes of games played. The value of moves early on in the sequence must be inferred indirectly from whether the game was won or lost.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "There are two more important attributes of the training experience to decide on: how much control does the learner have over training examples and how well does the training experience represent the distribution of *P*.\n",
      "\n",
      "Training data could be provided by the teacher, learner or a random process. If the learner is generating their own examples, are they making slight pertubations on promising molecules or novel ones? The distribution of the training set is analogous to force field parameterisation.\n",
      "\n",
      "The choice of learning setting has a large influence on the formulation of the algorithim.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Some (relevant) machine learning applications "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "USPEX"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Genetic/Evolutionary algorithims are loosely based on biological evolution. It begins with a starting population, which gives rise to the next through mutation or crossover, a part of the starting population is then replaced by this new generation. For CSP the genome of the population will include the characteristics of the crystal structure.\n",
      "\n",
      "USPEX is a evolutionary algorithm for inorganic CSP. The starting population is built up from the translation and packing of slices of whole unit cells/low energy motifs. After generation there are four \"variation operators\":  \n",
      "**Heredity** - creating a child structure from two parent slabs  \n",
      "**Lattice mutation** - a large deformation of the cell shape  \n",
      "**Permutation** - swapping chemically different atoms  \n",
      "**Special coordinate mutation** - Preferentially moving atoms with a low order and leaving high ones alone  \n",
      "\n",
      "There are a few other CSP approaches incorporating genetic algorithms such as MGAC and RANCEL."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Machine Learning for molecular electronic properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "There has been more than a few papers using machine learning to predict electronic properties, just highlighting 2 here that use entirely different approaches!\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<img src=\"Untitled.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Trained on 211 PAHs, with an assumed \"ideal\" face to face packing. Reference molecules were generated starting from benzene, adding additional rings at random bonds. Reorganisation energies ($\\lambda$) were calculated for every compound. They investigated several descriptors for correlation with $\\lambda$ and settled on a combination of molecular signature (a SMILES string of the molecule) and $\\Delta\\epsilon$ (change in HOMO eigenvalue).   \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}